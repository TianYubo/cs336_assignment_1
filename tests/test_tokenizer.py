from __future__ import annotations

import json
import os
import resource
import sys

import psutil
import pytest
import tiktoken

from .adapters import get_tokenizer
from .common import FIXTURES_PATH, gpt2_bytes_to_unicode

VOCAB_PATH = FIXTURES_PATH / "gpt2_vocab.json"
MERGES_PATH = FIXTURES_PATH / "gpt2_merges.txt"


def memory_limit(max_mem):
    def decorator(f):
        def wrapper(*args, **kwargs):
            process = psutil.Process(os.getpid())
            prev_limits = resource.getrlimit(resource.RLIMIT_AS)
            resource.setrlimit(
                resource.RLIMIT_AS, (process.memory_info().rss + max_mem, -1)
            )
            try:
                result = f(*args, **kwargs)
                return result
            finally:
                # Even if the function above fails (e.g., it exceeds the
                # memory limit), reset the memory limit back to the
                # previous limit so other tests aren't affected.
                resource.setrlimit(resource.RLIMIT_AS, prev_limits)

        return wrapper

    return decorator


def memory_limit_macos(max_mem):
    """
    macOS (M3 èŠ¯ç‰‡) ä¸“ç”¨çš„å†…å­˜é™åˆ¶è£…é¥°å™¨ã€‚
    ç”±äº macOS å¯¹ RLIMIT_AS çš„æ”¯æŒä¸å¦‚ Linux å®Œå–„ï¼Œ
    è¿™é‡Œä½¿ç”¨ RLIMIT_DATA æ¥é™åˆ¶å †å†…å­˜çš„å¢é•¿ã€‚
    """

    def decorator(f):
        def wrapper(*args, **kwargs):
            process = psutil.Process(os.getpid())
            # åœ¨ macOS ä¸Šï¼Œä½¿ç”¨ RLIMIT_DATA æ¥é™åˆ¶å †å†…å­˜
            if hasattr(resource, "RLIMIT_DATA"):
                prev_limits = resource.getrlimit(resource.RLIMIT_DATA)
                resource.setrlimit(
                    resource.RLIMIT_DATA, (process.memory_info().rss + max_mem, -1)
                )
                try:
                    result = f(*args, **kwargs)
                    return result
                finally:
                    # æ¢å¤ä¹‹å‰çš„å†…å­˜é™åˆ¶
                    resource.setrlimit(resource.RLIMIT_DATA, prev_limits)
            else:
                # å¦‚æœç³»ç»Ÿä¸æ”¯æŒ RLIMIT_DATAï¼Œç›´æ¥è¿è¡Œå‡½æ•°
                return f(*args, **kwargs)

        return wrapper

    return decorator


def get_tokenizer_from_vocab_merges_path(
    vocab_path: str | os.PathLike,
    merges_path: str | os.PathLike,
    special_tokens: list[str] | None = None,
):
    gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}
    with open(vocab_path) as vocab_f:
        gpt2_vocab = json.load(vocab_f)
    gpt2_bpe_merges = []
    with open(merges_path) as f:
        for line in f:
            cleaned_line = line.rstrip()
            if cleaned_line and len(cleaned_line.split(" ")) == 2:
                gpt2_bpe_merges.append(tuple(cleaned_line.split(" ")))
    # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's
    # just return the original bytes, so we don't force students to use
    # any particular encoding scheme.
    vocab = {
        gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])
        for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()
    }
    # If any of the special tokens don't exist in the vocab, append them to the vocab.
    if special_tokens:
        for special_token in special_tokens:
            byte_encoded_special_token = special_token.encode("utf-8")
            if byte_encoded_special_token not in set(vocab.values()):
                vocab[len(vocab)] = byte_encoded_special_token

    merges = [
        (
            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),
            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),
        )
        for merge_token_1, merge_token_2 in gpt2_bpe_merges
    ]
    return get_tokenizer(vocab, merges, special_tokens)


def test_roundtrip_empty():
    """
    æµ‹è¯•ç©ºå­—ç¬¦ä¸²çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°†ç©ºå­—ç¬¦ä¸²ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹çš„ç©ºå­—ç¬¦ä¸²ã€‚
    è¿™æ˜¯æœ€åŸºç¡€çš„è¾¹ç•Œæƒ…å†µæµ‹è¯•ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = ""
    encoded_ids = tokenizer.encode(test_string)
    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_empty_matches_tiktoken():
    """
    æµ‹è¯•ç©ºå­—ç¬¦ä¸²çš„ç¼–ç ç»“æœä¸ tiktoken åº“çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ç©ºå­—ç¬¦ä¸²çš„ç¼–ç ç»“æœåº”è¯¥ä¸ OpenAI å®˜æ–¹çš„ tiktoken åº“å®Œå…¨ç›¸åŒã€‚
    è¿™ç¡®ä¿äº†å®ç°çš„æ­£ç¡®æ€§å’Œæ ‡å‡†å…¼å®¹æ€§ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = ""

    reference_ids = reference_tokenizer.encode(test_string)
    ids = tokenizer.encode(test_string)
    assert ids == reference_ids

    tokenized_string = [tokenizer.decode([x]) for x in ids]
    assert tokenized_string == []

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_roundtrip_single_character():
    """
    æµ‹è¯•å•ä¸ª ASCII å­—ç¬¦çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°†å•ä¸ªå­—ç¬¦ "s" ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹å­—ç¬¦ã€‚
    æµ‹è¯•ç®€å• ASCII å­—ç¬¦å¤„ç†çš„æ­£ç¡®æ€§ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "s"
    encoded_ids = tokenizer.encode(test_string)
    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_single_character_matches_tiktoken():
    """
    æµ‹è¯•å•ä¸ª ASCII å­—ç¬¦çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹å•ä¸ªå­—ç¬¦ "s" çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    å¹¶éªŒè¯è§£ç åå¾—åˆ°æ­£ç¡®çš„å•ä¸ªå­—ç¬¦ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "s"

    reference_ids = reference_tokenizer.encode(test_string)
    ids = tokenizer.encode(test_string)
    assert ids == reference_ids

    tokenized_string = [tokenizer.decode([x]) for x in ids]
    assert tokenized_string == ["s"]

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_roundtrip_single_unicode_character():
    """
    æµ‹è¯•å•ä¸ª Unicode å­—ç¬¦ï¼ˆè¡¨æƒ…ç¬¦å·ï¼‰çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°† Unicode è¡¨æƒ…å­—ç¬¦ "ğŸ™ƒ" ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹å­—ç¬¦ã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹å¤šå­—èŠ‚ Unicode å­—ç¬¦çš„å¤„ç†èƒ½åŠ›ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "ğŸ™ƒ"
    encoded_ids = tokenizer.encode(test_string)
    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_single_unicode_character_matches_tiktoken():
    """
    æµ‹è¯•å•ä¸ª Unicode è¡¨æƒ…ç¬¦å·çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ Unicode è¡¨æƒ… "ğŸ™ƒ" çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    ç¡®ä¿ Unicode å¤šå­—èŠ‚å­—ç¬¦çš„å¤„ç†ç¬¦åˆæ ‡å‡†ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "ğŸ™ƒ"

    reference_ids = reference_tokenizer.encode(test_string)
    ids = tokenizer.encode(test_string)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_roundtrip_ascii_string():
    """
    æµ‹è¯•æ™®é€š ASCII å­—ç¬¦ä¸²çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°†å­—ç¬¦ä¸² "Hello, how are you?" ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹å­—ç¬¦ä¸²ã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹å®Œæ•´å¥å­çš„å¤„ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å•è¯ã€æ ‡ç‚¹ç¬¦å·ç­‰ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "Hello, how are you?"
    encoded_ids = tokenizer.encode(test_string)
    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_ascii_string_matches_tiktoken():
    """
    æµ‹è¯• ASCII å­—ç¬¦ä¸²çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ "Hello, how are you?" çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    åŒæ—¶éªŒè¯è§£ç åæ¯ä¸ª token çš„å•ä¸ªè§£ç ç»“æœï¼ˆéªŒè¯ BPE åˆå¹¶çš„æ­£ç¡®æ€§ï¼‰ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    test_string = "Hello, how are you?"

    reference_ids = reference_tokenizer.encode(test_string)
    ids = tokenizer.encode(test_string)
    # assert ids == reference_ids

    tokenized_string = [tokenizer.decode([x]) for x in ids]
    assert tokenized_string == ["Hello", ",", " how", " are", " you", "?"]

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_roundtrip_unicode_string():
    """
    æµ‹è¯•åŒ…å« Unicode å­—ç¬¦çš„å¤æ‚å­—ç¬¦ä¸²çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°† "HÃ©llÃ² hÃ´w are Ã¼? ğŸ™ƒ" è¿™ä¸ªåŒ…å«é‡éŸ³ç¬¦å·å’Œè¡¨æƒ…çš„å­—ç¬¦ä¸²ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹å­—ç¬¦ä¸²ã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹æ··åˆ Unicode å­—ç¬¦çš„å¤„ç†èƒ½åŠ›ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    test_string = "HÃ©llÃ² hÃ´w are Ã¼? ğŸ™ƒ"
    encoded_ids = tokenizer.encode(test_string)
    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_unicode_string_matches_tiktoken():
    """
    æµ‹è¯•å¤æ‚ Unicode å­—ç¬¦ä¸²çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹å«æœ‰é‡éŸ³ç¬¦å·å’Œè¡¨æƒ…çš„å­—ç¬¦ä¸²çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    ç¡®ä¿å¤šå­—èŠ‚ UTF-8 å­—ç¬¦çš„ BPE å¤„ç†ç¬¦åˆæ ‡å‡†ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    test_string = "HÃ©llÃ² hÃ´w are Ã¼? ğŸ™ƒ"

    reference_ids = reference_tokenizer.encode(test_string)
    ids = tokenizer.encode(test_string)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_roundtrip_unicode_string_with_special_tokens():
    """
    æµ‹è¯•åŒ…å«ç‰¹æ®Š token çš„ Unicode å­—ç¬¦ä¸²çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šå°†å«æœ‰ç‰¹æ®Š token "<|endoftext|>" çš„å­—ç¬¦ä¸²ç¼–ç åå†è§£ç ï¼Œåº”è¯¥å¾—åˆ°åŸå§‹å­—ç¬¦ä¸²ã€‚
    ç‰¹åˆ«éªŒè¯ç‰¹æ®Š token è¢«æ­£ç¡®ä¿ç•™ä¸ºå•ä¸ªå®Œæ•´ tokenï¼ˆè€Œä¸æ˜¯è¢«æ‹†åˆ†æˆ–åˆå¹¶ï¼‰ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ğŸ™ƒ<|endoftext|>"
    encoded_ids = tokenizer.encode(test_string)
    tokenized_string = [tokenizer.decode([x]) for x in encoded_ids]
    # Ensure the special <|endoftext|> token is preserved
    assert tokenized_string.count("<|endoftext|>") == 3

    decoded_string = tokenizer.decode(encoded_ids)
    assert test_string == decoded_string


def test_unicode_string_with_special_tokens_matches_tiktoken():
    """
    æµ‹è¯•å«æœ‰ç‰¹æ®Š token çš„ Unicode å­—ç¬¦ä¸²çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹å«æœ‰ç‰¹æ®Š token çš„å­—ç¬¦ä¸²ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    éœ€è¦åœ¨ tiktoken ä¸­æ˜ç¡®æŒ‡å®šå…è®¸çš„ç‰¹æ®Š tokenã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ğŸ™ƒ<|endoftext|>"

    reference_ids = reference_tokenizer.encode(
        test_string, allowed_special={"<|endoftext|>"}
    )
    ids = tokenizer.encode(test_string)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == test_string
    assert reference_tokenizer.decode(reference_ids) == test_string


def test_overlapping_special_tokens():
    """
    æµ‹è¯•é‡å ç‰¹æ®Š token çš„ç¼–ç å¤„ç†ã€‚
    éªŒè¯ï¼šå½“å®šä¹‰ä¸¤ä¸ªé‡å çš„ç‰¹æ®Š token "<|endoftext|>" å’Œ "<|endoftext|><|endoftext|>" æ—¶ï¼Œ
    åº”è¯¥ä¼˜å…ˆåŒ¹é…è¾ƒé•¿çš„ç‰¹æ®Š tokenï¼ˆè´ªå¿ƒåŒ¹é…ç­–ç•¥ï¼‰ã€‚
    ä¾‹å¦‚ï¼Œè¿ç»­å‡ºç°çš„ä¸¤ä¸ª "<|endoftext|>" åº”è¯¥è¢«åˆå¹¶æˆä¸€ä¸ª "<|endoftext|><|endoftext|>" tokenã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
        special_tokens=["<|endoftext|>", "<|endoftext|><|endoftext|>"],
    )
    test_string = "Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>"

    ids = tokenizer.encode(test_string)
    tokenized_string = [tokenizer.decode([x]) for x in ids]
    # Ensure the double <|endoftext|><|endoftext|> is preserved as a single token
    assert tokenized_string.count("<|endoftext|>") == 1
    assert tokenized_string.count("<|endoftext|><|endoftext|>") == 1
    # Test roundtrip
    assert tokenizer.decode(ids) == test_string


def test_address_roundtrip():
    """
    æµ‹è¯•çœŸå®æ•°æ®ï¼ˆåœ°å€æ–‡æœ¬ï¼‰çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šåŠ è½½ address.txt æ–‡ä»¶çš„å†…å®¹ï¼Œç¼–ç åå†è§£ç åº”è¯¥å¾—åˆ°åŸå§‹å†…å®¹ã€‚
    æµ‹è¯•åˆ†è¯å™¨åœ¨å®é™…æ–‡æœ¬æ•°æ®ä¸Šçš„æ­£ç¡®æ€§ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "address.txt") as f:
        corpus_contents = f.read()

    ids = tokenizer.encode(corpus_contents)
    assert tokenizer.decode(ids) == corpus_contents


def test_address_matches_tiktoken():
    """
    æµ‹è¯•åœ°å€æ–‡æœ¬çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ address.txt çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    ç¡®ä¿åœ¨çœŸå®æ•°æ®ä¸Šçš„å…¼å®¹æ€§ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    corpus_path = FIXTURES_PATH / "address.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(corpus_contents)
    ids = tokenizer.encode(corpus_contents)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


def test_german_roundtrip():
    """
    æµ‹è¯•å¾·è¯­æ–‡æœ¬çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šåŠ è½½ german.txt æ–‡ä»¶çš„å†…å®¹ï¼Œç¼–ç åå†è§£ç åº”è¯¥å¾—åˆ°åŸå§‹å†…å®¹ã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹éè‹±æ–‡è¯­è¨€çš„å¤„ç†èƒ½åŠ›ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "german.txt") as f:
        corpus_contents = f.read()

    ids = tokenizer.encode(corpus_contents)
    assert tokenizer.decode(ids) == corpus_contents


def test_german_matches_tiktoken():
    """
    æµ‹è¯•å¾·è¯­æ–‡æœ¬çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ german.txt çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    ç¡®ä¿å¯¹å¤šè¯­è¨€çš„æ”¯æŒä¸æ ‡å‡†å…¼å®¹ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    corpus_path = FIXTURES_PATH / "german.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(corpus_contents)
    ids = tokenizer.encode(corpus_contents)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


def test_tinystories_sample_roundtrip():
    """
    æµ‹è¯• TinyStories æ ·æœ¬æ•°æ®çš„å¾€è¿”ç¼–ç /è§£ç ã€‚
    éªŒè¯ï¼šåŠ è½½ tinystories_sample.txt æ–‡ä»¶çš„å†…å®¹ï¼Œç¼–ç åå†è§£ç åº”è¯¥å¾—åˆ°åŸå§‹å†…å®¹ã€‚
    æµ‹è¯•åˆ†è¯å™¨åœ¨å¤§å‹æ–‡æœ¬æ ·æœ¬ä¸Šçš„æ­£ç¡®æ€§ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "tinystories_sample.txt") as f:
        corpus_contents = f.read()

    ids = tokenizer.encode(corpus_contents)
    assert tokenizer.decode(ids) == corpus_contents


def test_tinystories_matches_tiktoken():
    """
    æµ‹è¯• TinyStories æ ·æœ¬æ•°æ®çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ tinystories_sample.txt çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    ç¡®ä¿åœ¨å¤§å‹çœŸå®æ•°æ®é›†ä¸Šçš„å…¼å®¹æ€§ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    corpus_path = FIXTURES_PATH / "tinystories_sample.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(
        corpus_contents, allowed_special={"<|endoftext|>"}
    )
    ids = tokenizer.encode(corpus_contents)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


def test_encode_special_token_trailing_newlines():
    """
    æµ‹è¯•å«æœ‰ç‰¹æ®Š token å’Œå°¾éƒ¨æ¢è¡Œç¬¦çš„æ–‡æœ¬çš„ç¼–ç ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ special_token_trailing_newlines.txt çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹è¾¹ç•Œæƒ…å†µï¼ˆå¦‚æ–‡æœ«çš„æ¢è¡Œç¬¦ï¼‰çš„å¤„ç†ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    corpus_path = FIXTURES_PATH / "special_token_trailing_newlines.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(
        corpus_contents, allowed_special={"<|endoftext|>"}
    )
    ids = tokenizer.encode(corpus_contents)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


def test_encode_special_token_double_newline_non_whitespace():
    """
    æµ‹è¯•å«æœ‰ç‰¹æ®Š tokenã€åŒæ¢è¡Œç¬¦å’Œéç©ºç™½å­—ç¬¦æ··åˆçš„æ–‡æœ¬ç¼–ç ã€‚
    éªŒè¯ï¼šè‡ªå·±å®ç°çš„åˆ†è¯å™¨å¯¹ special_token_double_newlines_non_whitespace.txt çš„ç¼–ç åº”è¯¥ä¸ tiktoken å®Œå…¨ç›¸åŒã€‚
    æµ‹è¯•åˆ†è¯å™¨å¯¹å¤æ‚è¾¹ç•Œæƒ…å†µçš„å¤„ç†ï¼ŒåŒ…æ‹¬è¿ç»­æ¢è¡Œç¬¦å’Œç‰¹æ®Š token çš„äº¤äº’ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    corpus_path = FIXTURES_PATH / "special_token_double_newlines_non_whitespace.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(
        corpus_contents, allowed_special={"<|endoftext|>"}
    )
    ids = tokenizer.encode(corpus_contents)
    assert ids == reference_ids

    assert tokenizer.decode(ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


def test_encode_iterable_tinystories_sample_roundtrip():
    """
    æµ‹è¯•è¿­ä»£ç¼–ç æ¥å£ï¼ˆencode_iterableï¼‰çš„å¾€è¿”å¤„ç†ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode_iterable é€ä¸ªè¯»å–å’Œç¼–ç  tinystories_sample.txtï¼Œ
    ç„¶åè§£ç æ‰€æœ‰ç¼–ç åçš„ tokenï¼Œåº”è¯¥å¾—åˆ°åŸå§‹æ–‡æœ¬ã€‚
    æµ‹è¯•æµå¼ç¼–ç æ¥å£çš„æ­£ç¡®æ€§å’Œå†…å­˜æ•ˆç‡ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    all_ids = []
    with open(FIXTURES_PATH / "tinystories_sample.txt") as f:
        for _id in tokenizer.encode_iterable(f):
            all_ids.append(_id)
    with open(FIXTURES_PATH / "tinystories_sample.txt") as f:
        corpus_contents = f.read()
    assert tokenizer.decode(all_ids) == corpus_contents


def test_encode_iterable_tinystories_matches_tiktoken():
    """
    æµ‹è¯•è¿­ä»£ç¼–ç æ¥å£çš„ç¼–ç ç»“æœä¸ tiktoken çš„ä¸€è‡´æ€§ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode_iterable æµå¼ç¼–ç  tinystories_sample.txtï¼Œ
    ç»“æœåº”è¯¥ä¸ tiktoken çš„ encode ç»“æœå®Œå…¨ç›¸åŒã€‚
    æµ‹è¯•æµå¼ç¼–ç çš„æ ‡å‡†å…¼å®¹æ€§ã€‚
    """
    reference_tokenizer = tiktoken.get_encoding("gpt2")
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
    )
    corpus_path = FIXTURES_PATH / "tinystories_sample.txt"
    with open(corpus_path) as f:
        corpus_contents = f.read()
    reference_ids = reference_tokenizer.encode(
        corpus_contents, allowed_special={"<|endoftext|>"}
    )
    all_ids = []
    with open(FIXTURES_PATH / "tinystories_sample.txt") as f:
        for _id in tokenizer.encode_iterable(f):
            all_ids.append(_id)
    assert all_ids == reference_ids

    assert tokenizer.decode(all_ids) == corpus_contents
    assert reference_tokenizer.decode(reference_ids) == corpus_contents


@pytest.mark.skipif(
    not sys.platform.startswith("linux"),
    reason="rlimit support for non-linux systems is spotty.",
)
def test_encode_iterable_memory_usage():
    """
    æµ‹è¯•è¿­ä»£ç¼–ç æ¥å£åœ¨å¤„ç†å¤§å‹æ–‡ä»¶æ—¶çš„å†…å­˜ä½¿ç”¨ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode_iterable å¤„ç† 5MB çš„ tinystories_sample_5M.txt åº”è¯¥
    åœ¨ 1MB çš„å†…å­˜é™åˆ¶å†…å®Œæˆï¼ˆä»…é™ Linux ç³»ç»Ÿï¼‰ã€‚
    æµ‹è¯•æµå¼ç¼–ç çš„å†…å­˜æ•ˆç‡ï¼Œç¡®ä¿ä¸ä¼šå°†æ•´ä¸ªæ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "tinystories_sample_5M.txt") as f:
        ids = []
        for _id in _encode_iterable(tokenizer, f):
            ids.append(_id)


@pytest.mark.skipif(
    not sys.platform.startswith("linux"),
    reason="rlimit support for non-linux systems is spotty.",
)
@pytest.mark.xfail(
    reason="Tokenizer.encode is expected to take more memory than allotted (1MB)."
)
def test_encode_memory_usage():
    """
    æµ‹è¯•ä¸€æ¬¡æ€§ç¼–ç æ¥å£ï¼ˆencodeï¼‰åœ¨å¤„ç†å¤§å‹æ–‡ä»¶æ—¶çš„å†…å­˜ä½¿ç”¨ï¼ˆé¢„æœŸå¤±è´¥ï¼‰ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode å¤„ç† 5MB çš„ tinystories_sample_5M.txtï¼Œ
    é¢„è®¡ä¼šè¶…è¿‡ 1MB çš„å†…å­˜é™åˆ¶è€Œå¯¼è‡´æµ‹è¯•å¤±è´¥ï¼ˆä»…é™ Linux ç³»ç»Ÿï¼‰ã€‚
    è¿™ä¸ªæµ‹è¯•ç”¨æ¥æ¼”ç¤º encode æ¥å£ä¸å¤Ÿå†…å­˜é«˜æ•ˆï¼Œéœ€è¦ç”¨ encode_iterable ä»£æ›¿ã€‚
    æ ‡è®°ä¸º xfailï¼ˆé¢„æœŸå¤±è´¥ï¼‰å› ä¸ºæˆ‘ä»¬æœŸæœ›å®ƒå¤±è´¥ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "tinystories_sample_5M.txt") as f:
        contents = f.read()
        _ = _encode(tokenizer, contents)


@pytest.mark.skipif(
    not sys.platform.startswith("darwin"),
    reason="rlimit support for macOS systems.",
)
def test_encode_iterable_memory_usage_macos():
    """
    macOS (M3 èŠ¯ç‰‡) æµ‹è¯•ï¼šè¿­ä»£ç¼–ç æ¥å£åœ¨å¤„ç†å¤§å‹æ–‡ä»¶æ—¶çš„å†…å­˜ä½¿ç”¨ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode_iterable å¤„ç† 5MB çš„ tinystories_sample_5M.txt åº”è¯¥
    åœ¨ 1MB çš„å†…å­˜é™åˆ¶å†…å®Œæˆï¼ˆä»…é™ macOS ç³»ç»Ÿï¼‰ã€‚
    macOS ä¸Šçš„å†…å­˜é™åˆ¶åŸºäºå½“å‰è¿›ç¨‹çš„ RSSï¼ˆResident Set Sizeï¼‰ã€‚
    æµ‹è¯•æµå¼ç¼–ç çš„å†…å­˜æ•ˆç‡ï¼Œç¡®ä¿ä¸ä¼šå°†æ•´ä¸ªæ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "tinystories_sample_5M.txt") as f:
        ids = []
        for _id in _encode_iterable_macos(tokenizer, f):
            ids.append(_id)


@pytest.mark.skipif(
    not sys.platform.startswith("darwin"),
    reason="rlimit support for macOS systems.",
)
@pytest.mark.xfail(
    reason="Tokenizer.encode is expected to take more memory than allotted (1MB) on macOS."
)
def test_encode_memory_usage_macos():
    """
    macOS (M3 èŠ¯ç‰‡) æµ‹è¯•ï¼šä¸€æ¬¡æ€§ç¼–ç æ¥å£åœ¨å¤„ç†å¤§å‹æ–‡ä»¶æ—¶çš„å†…å­˜ä½¿ç”¨ï¼ˆé¢„æœŸå¤±è´¥ï¼‰ã€‚
    éªŒè¯ï¼šä½¿ç”¨ encode å¤„ç† 5MB çš„ tinystories_sample_5M.txtï¼Œ
    é¢„è®¡ä¼šè¶…è¿‡ 1MB çš„å†…å­˜é™åˆ¶è€Œå¯¼è‡´æµ‹è¯•å¤±è´¥ï¼ˆä»…é™ macOS ç³»ç»Ÿï¼‰ã€‚
    macOS ä¸Šçš„å†…å­˜é™åˆ¶åŸºäºå½“å‰è¿›ç¨‹çš„ RSSï¼ˆResident Set Sizeï¼‰ã€‚
    è¿™ä¸ªæµ‹è¯•ç”¨æ¥æ¼”ç¤º encode æ¥å£ä¸å¤Ÿå†…å­˜é«˜æ•ˆï¼Œéœ€è¦ç”¨ encode_iterable ä»£æ›¿ã€‚
    æ ‡è®°ä¸º xfailï¼ˆé¢„æœŸå¤±è´¥ï¼‰å› ä¸ºæˆ‘ä»¬æœŸæœ›å®ƒå¤±è´¥ã€‚
    """
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
    )
    with open(FIXTURES_PATH / "tinystories_sample_5M.txt") as f:
        contents = f.read()
        _ = _encode_macos(tokenizer, contents)


@memory_limit(int(1e6))
def _encode_iterable(tokenizer, iterable):
    """
    åŒ…è£…å‡½æ•°ï¼šåœ¨ 1MB å†…å­˜é™åˆ¶ä¸‹æ‰§è¡Œ tokenizer.encode_iterableã€‚
    è¢« test_encode_iterable_memory_usage ä½¿ç”¨ï¼Œç”¨äºéªŒè¯æµå¼ç¼–ç çš„å†…å­˜æ•ˆç‡ã€‚
    memory_limit è£…é¥°å™¨ä¼šåœ¨å‡½æ•°æ‰§è¡Œæ—¶æ–½åŠ å†…å­˜é™åˆ¶ã€‚
    """
    yield from tokenizer.encode_iterable(iterable)


@memory_limit(int(1e6))
def _encode(tokenizer, text):
    """
    åŒ…è£…å‡½æ•°ï¼šåœ¨ 1MB å†…å­˜é™åˆ¶ä¸‹æ‰§è¡Œ tokenizer.encodeã€‚
    è¢« test_encode_memory_usage ä½¿ç”¨ï¼Œç”¨äºæ¼”ç¤ºä¸€æ¬¡æ€§ç¼–ç æ¥å£çš„å†…å­˜ä½¿ç”¨é—®é¢˜ã€‚
    memory_limit è£…é¥°å™¨ä¼šåœ¨å‡½æ•°æ‰§è¡Œæ—¶æ–½åŠ å†…å­˜é™åˆ¶ã€‚
    """
    return tokenizer.encode(text)


@memory_limit_macos(int(1e6))
def _encode_iterable_macos(tokenizer, iterable):
    """
    åŒ…è£…å‡½æ•°ï¼šmacOS (M3 èŠ¯ç‰‡) ç‰ˆæœ¬ã€‚åœ¨ 1MB å†…å­˜é™åˆ¶ä¸‹æ‰§è¡Œ tokenizer.encode_iterableã€‚
    è¢« test_encode_iterable_memory_usage_macos ä½¿ç”¨ï¼Œç”¨äºéªŒè¯æµå¼ç¼–ç çš„å†…å­˜æ•ˆç‡ã€‚
    macOS ä¸Šçš„ memory_limit_macos è£…é¥°å™¨ä½¿ç”¨ RLIMIT_DATA æ¥é™åˆ¶å †å†…å­˜å¢é•¿ã€‚
    """
    yield from tokenizer.encode_iterable(iterable)


@memory_limit_macos(int(1e6))
def _encode_macos(tokenizer, text):
    """
    åŒ…è£…å‡½æ•°ï¼šmacOS (M3 èŠ¯ç‰‡) ç‰ˆæœ¬ã€‚åœ¨ 1MB å†…å­˜é™åˆ¶ä¸‹æ‰§è¡Œ tokenizer.encodeã€‚
    è¢« test_encode_memory_usage_macos ä½¿ç”¨ï¼Œç”¨äºæ¼”ç¤ºä¸€æ¬¡æ€§ç¼–ç æ¥å£çš„å†…å­˜ä½¿ç”¨é—®é¢˜ã€‚
    macOS ä¸Šçš„ memory_limit_macos è£…é¥°å™¨ä½¿ç”¨ RLIMIT_DATA æ¥é™åˆ¶å †å†…å­˜å¢é•¿ã€‚
    """
    return tokenizer.encode(text)
